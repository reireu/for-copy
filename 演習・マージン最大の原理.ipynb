{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rena-tech/for-copy/blob/main/%E6%BC%94%E7%BF%92%E3%83%BB%E3%83%9E%E3%83%BC%E3%82%B7%E3%82%99%E3%83%B3%E6%9C%80%E5%A4%A7%E3%81%AE%E5%8E%9F%E7%90%86.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac821db",
      "metadata": {
        "id": "6ac821db"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from matplotlib import animation, rc\n",
        "import matplotlib.patches as patches\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "import pickle\n",
        "import sympy as sp\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a501b1f3",
      "metadata": {
        "id": "a501b1f3"
      },
      "source": [
        "# マージン最大の原理\n",
        "以下のプログラムで生成される訓練データを**線形分類器**により分類することを考えます。\n",
        "\n",
        "> #### 線形分類器\n",
        "2種類のラベルが与えられた$n$次元の訓練データを分離する$(n-1)$次元の超平面を探す分類アルゴリズムです。\n",
        "例えば、ラベルを$-1$と$+1$とすると、\n",
        "線形分類器は係数$a_0, \\dots, a_n$を探し出して、大多数の訓練データに対して、\n",
        ">- データ$(x_1, \\dots, x_n)$のラベルが$-1$ならば、$a_0 + a_1x_1 + \\dots + a_nx_n < 0$\n",
        ">- データ$(x_1, \\dots, x_n)$のラベルが$＋1$ならば、$a_0 + a_1x_1 + \\dots + a_nx_n > 0$\n",
        ">\n",
        "> が成り立つような$a_0, \\dots, a_n$を探索します。\n",
        ">\n",
        "> 線形分類器の代表的な例は、**ロジスティック回帰（Logistic Regression）**、\n",
        "**パーセプトロン（Perceptron）**, **SVC（Support Vector Machine）**などです。\n",
        "\n",
        "```python\n",
        "nSample = np.random.rand(5,2) + [-1,0]\n",
        "pSample = np.random.rand(5,2) + [1,0]\n",
        "print(nSample)\n",
        "print(pSample)\n",
        "neg = np.concatenate([sd * np.random.randn(100, 2) + pt for pt in nSample])\n",
        "pos = np.concatenate([sd * np.random.randn(100, 2) + pt for pt in pSample])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f7d514",
      "metadata": {
        "scrolled": true,
        "id": "92f7d514"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "30f70dac",
      "metadata": {
        "id": "30f70dac"
      },
      "source": [
        "上記のプログラムで生成した訓練データは2次元データですので、\n",
        "線形分類器は直線$a_0 + a_1 x + a_2 y = 0$で青のプロット（$+1$）と赤のプロット（$-1$）を分離します。\n",
        "\n",
        "例えば、パーセプトロン（sklearn.linear_model.Perceptron）を使って、\n",
        "分離直線を探してみましょう。\n",
        "```python\n",
        "from sklearn.linear_model import Perceptron\n",
        "model = Perceptron()\n",
        "\n",
        "train = np.concatenate([nSample,pSample])\n",
        "label = [-1]*len(nSample) + [1]*len(pSample)\n",
        "model.fit(train, label)\n",
        "ymin = min(min(nSample[:,1]), min(pSample[:,1]))\n",
        "ymax = max(max(nSample[:,1]), max(pSample[:,1]))\n",
        "plt.scatter(nSample[:,0], nSample[:,1], c='r')\n",
        "plt.scatter(pSample[:,0], pSample[:,1], c='b')\n",
        "\n",
        "plt.plot((-np.array([ymin, ymax])*model.coef_[0,1]-model.intercept_)/model.coef_[0,0], [ymin, ymax], 'g-')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dacca3ad",
      "metadata": {
        "scrolled": true,
        "id": "dacca3ad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "120c3a54",
      "metadata": {
        "id": "120c3a54"
      },
      "source": [
        "### 演習\n",
        "線形分類器を、\n",
        "ロジスティック回帰（sklearn.linear_model.LogisticRegression）、\n",
        "SVM（sklearn.svm.LinearSVC）に変更して、\n",
        "同様に分離直線を描画せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c3848f",
      "metadata": {
        "id": "10c3848f"
      },
      "outputs": [],
      "source": [
        "# ロジスティック回帰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f44930",
      "metadata": {
        "id": "62f44930"
      },
      "outputs": [],
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98590912",
      "metadata": {
        "id": "98590912"
      },
      "source": [
        "分類器によって求められる分離直線が異なることが観察できます。\n",
        "\n",
        "つまり、訓練データを分離する分離直線は複数存在するのです。\n",
        "\n",
        "では、複数存在する分離直線の中から最良のものを選択するにはどうすればよいのでしょうか？\n",
        "\n",
        "以下では、よい分類直線（超平面）を決定する原理として、\n",
        "**マージン最大の原理**について説明します。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ddac9dd",
      "metadata": {
        "id": "4ddac9dd"
      },
      "source": [
        "## 1匹いたら100匹いると思え\n",
        "ゴキブリの話ではありませんが、本質は同じです。\n",
        "\n",
        "訓練データはサンプルです。\n",
        "たまたま見かけるゴキブリもサンプルです。\n",
        "1個の訓練データの周辺には多くのデータが存在する可能性が高いと考えるべきです。\n",
        "\n",
        "例えば、\n",
        "以下のプログラムは、\n",
        "先の訓練データの陰に隠れている「真の」データを薄い赤と薄い青でプロットしたものです。\n",
        "濃い赤と濃い青でプロットされたサンプル（訓練データ）の周辺に\n",
        "多くの「真の」データがかくれていることが分かります。\n",
        "\n",
        "```python\n",
        "sd = 0.5\n",
        "neg = np.concatenate([sd * np.random.randn(100, 2) + pt for pt in nSample])\n",
        "pos = np.concatenate([sd * np.random.randn(100, 2) + pt for pt in pSample])\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "model = Perceptron()\n",
        "model.fit(train, label)\n",
        "\n",
        "plt.Figure(figsize=(10,10))\n",
        "plt.scatter(nSample[:,0], nSample[:,1], c='r')\n",
        "plt.scatter(pSample[:,0], pSample[:,1], c='b')\n",
        "plt.scatter(neg[:,0], neg[:,1], c='r', alpha=0.2, s=5)\n",
        "plt.scatter(pos[:,0], pos[:,1], c='b', alpha=0.2, s=5)\n",
        "ymin = min(min(neg[:,1]), min(pos[:,1]))\n",
        "ymax = max(max(neg[:,1]), max(pos[:,1]))\n",
        "plt.plot((-np.array([ymin, ymax])*model.coef_[0,1]-model.intercept_)/model.coef_[0,0], [ymin, ymax], 'g-')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cb1e26a",
      "metadata": {
        "id": "7cb1e26a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9b2ee843",
      "metadata": {
        "id": "9b2ee843"
      },
      "source": [
        "パーセプトロンによって得られた分離直線が、\n",
        "真のデータをどのくらい正確に分類しているかを計算してみます。\n",
        "```python\n",
        "def accuracy(coef, intercept):\n",
        "    nHit = len([pt for pt in neg if np.dot(coef, pt) + intercept < 0])\n",
        "    pHit = len([pt for pt in pos if np.dot(coef, pt) + intercept > 0])\n",
        "    return (nHit + pHit)/(len(neg) + len(pos))\n",
        "\n",
        "print(accuracy(model.coef_, model.intercept_))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde82969",
      "metadata": {
        "id": "cde82969"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dc774251",
      "metadata": {
        "id": "dc774251"
      },
      "source": [
        "かなり正確に分類が出来ていると思いますが、\n",
        "果たして最良でしょうか？\n",
        "\n",
        "確かめて見ましょう。\n",
        "\n",
        "次のプログラムを実行すると、\n",
        "パーセプトロンが選択した分離直線が緑の線で表され、\n",
        "赤の訓練データと青の訓練データを**丁度真ん中**で分離する直線が黒の破線で表されます。\n",
        "\n",
        "アニメーションを実行すると、\n",
        "黒の実戦で表現される分離直線が平行に移動して、\n",
        "その時々の**真のデータに対する正解率**がどのように変化するかグラフで示されます。\n",
        "\n",
        "```python\n",
        "coef = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "hi_lim = min([-np.dot(coef, pt) for pt in nSample])\n",
        "lo_lim = max([-np.dot(coef, pt) for pt in pSample])\n",
        "\n",
        "mid = (hi_lim + lo_lim)/2\n",
        "\n",
        "intercepts = np.linspace(lo_lim, hi_lim, 51)\n",
        "accs = [accuracy(coef, c) for c in intercepts]\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(8,9))\n",
        "axes[0].scatter(nSample[:,0], nSample[:,1], c='r')\n",
        "axes[0].scatter(pSample[:,0], pSample[:,1], c='b')\n",
        "axes[0].scatter(neg[:,0], neg[:,1], c='r', alpha=0.2, s=5)\n",
        "axes[0].scatter(pos[:,0], pos[:,1], c='b', alpha=0.2, s=5)\n",
        "axes[0].plot((-np.array([ymin, ymax])*coef[1]-intercept)/coef[0], [ymin, ymax], 'g-')\n",
        "axes[0].plot((-np.array([ymin, ymax])*coef[1]-mid)/coef[0], [ymin, ymax], 'k--')\n",
        "axes[0].grid()\n",
        "axes[1].plot([mid, mid], [min(accs), max(accs)], 'k--')\n",
        "axes[1].plot([intercept, intercept], [min(accs), max(accs)], 'g-')\n",
        "axes[1].xlabel()\n",
        "axes[1].grid()\n",
        "\n",
        "ims = []\n",
        "\n",
        "for i in range(len(intercepts)):\n",
        "    c = intercepts[i]\n",
        "    im = axes[0].plot((-np.array([ymin, ymax])*coef[1] - c)/coef[0], [ymin, ymax], 'k-')\n",
        "    im += axes[1].plot(intercepts[:i+1], accs[:i+1], 'k-')\n",
        "    ims.append(im)\n",
        "    \n",
        "ani = animation.ArtistAnimation(fig, ims, interval=200)\n",
        "rc('animation', html='jshtml')\n",
        "plt.close()\n",
        "display(ani)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c4bf89",
      "metadata": {
        "id": "d5c4bf89"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "86909e5c",
      "metadata": {
        "id": "86909e5c"
      },
      "source": [
        "正解率は最大に近い値を示す時、\n",
        "分離直線は、黒の破線で示した**丁度真ん中**の直線に近いことが分かります。\n",
        "\n",
        "このことは次のように説明することができます。\n",
        "\n",
        "- 分離直線が赤の訓練データP（ラベル$-1$）の近くを通るとき、\n",
        "Pの周辺に分布する多数の赤の真のデータを左右に分離してしまいます\n",
        "- 分離直線が青の訓練データP（ラベル$+1$）の近くを通るとき、\n",
        "Pの周辺に分布する多数の青の真のデータを左右に分離してしまいます。\n",
        "\n",
        "分離直線が多くの同色の点を分離すると、不正解のデータが多く発生し、\n",
        "正解率が下がります。\n",
        "\n",
        "つまり、色を問わず、**訓練データから最も遠くなるように分離直線を選ぶ**ことで、\n",
        "「真のデータ」に対する正解率が最大になることを期待することができ、\n",
        "特に、赤の訓練データと青の訓練データの**丁度真ん中**を通るように\n",
        "分離直線を選ぶことが望ましいわけです。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c3f9851",
      "metadata": {
        "id": "2c3f9851"
      },
      "source": [
        "今度は、\n",
        "訓練データを固定したまま訓練データの周りに分布する「真のデータ」をランダムに変化させて、\n",
        "分離直線が丁度真ん中を通る時（訓練データから最も遠くなる時）に正解率が最大になる確率が高いことを確認してみます。\n",
        "\n",
        "次のプログラムは、\n",
        "ランダムに生成した「真のデータ」に対して、\n",
        "パーセプトロンが計算した分離直線を平行に移動して（傾きはそのままで切片を変化させて）、\n",
        "最大の正解率を示す切片の値を求めます。\n",
        "1000通りの真のデータ分布を生成して、それぞれについて最大正解率を示す切片を記録し、\n",
        "分離直線が赤の訓練データと青の訓練データの丁度真ん中に来るときに（黒の破線）最大正解率を得る\n",
        "確率が最も高くなることを確かめます。\n",
        "\n",
        "```python\n",
        "sd = 0.5\n",
        "N = 51\n",
        "intercepts = np.linspace(lo_lim, hi_lim, N)\n",
        "mid = (lo_lim + hi_lim)/2\n",
        "count = [0] * N\n",
        "\n",
        "for _ in range(1000):\n",
        "    neg = np.concatenate([sd * np.random.randn(100, 2) + pt for pt in nSample])\n",
        "    pos = np.concatenate([sd * np.random.randn(100, 2) + pt for pt in pSample])\n",
        "    accs = [accuracy(coef, c) for c in np.linspace(lo_lim, hi_lim, 51)]\n",
        "    count[np.argmax(accs)] += 1\n",
        "    \n",
        "plt.plot(intercepts, count)\n",
        "plt.plot([mid, mid], [0, max(count)], 'k--')\n",
        "plt.xlabel('切片')\n",
        "plt.ylabel('最大の正解率を示す頻度')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9189b0ed",
      "metadata": {
        "id": "9189b0ed"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "27f491cb",
      "metadata": {
        "id": "27f491cb"
      },
      "source": [
        "## マージン最大化の原理\n",
        "\n",
        "分離超平面（分離直線）から最も近い訓練データまでの距離を**マージン**と呼びます。\n",
        "\n",
        "ここまでの観察から、**マージンが大きいほど高い正解率を期待できる**ことが分かりました。\n",
        "\n",
        "「マージンを最大化するように分離超平面（分離直線）を選ぶ」という分類器の設計指針を**マージン最大化の原理**と呼びます。\n",
        "\n",
        "SVMはマージン最大化の原理に基づく分類器です。\n",
        "\n",
        "#### 演習\n",
        "sklearn.svm.LinearSVCをハイパーパラメータ`C=1000`で学習させて得られる分離曲線が、\n",
        "赤の訓練データと青の訓練データと等距離であることを確認せよ。\n",
        "\n",
        "**ヒント**\n",
        "\n",
        "- アニメーションのプログラムの先頭にLinearSVCによる学習のコードを挿入する。\n",
        "- `model = LinearSVC(C=1000)`により初期モデルの生成を行う。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811397a2",
      "metadata": {
        "id": "811397a2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdc858e",
      "metadata": {
        "id": "8bdc858e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}